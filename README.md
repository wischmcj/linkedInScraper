## LinkedIn Scraper

A practical LinkedIn data scraper and pipeline based on dlt (data load tool), DuckDB, and lightweight request clients. Created by reverse-engineering LinkedIn's 'hidden' Voyager API endpoints (GraphQL/REST) with cookie-based auth. It focuses on extracting jobs posted for followed companies.

## Key Dependencies 
- DLT
- DuckDB
- Streamlit (optional)

### Basic capabilities
  - Easy extraction of data from LinkedIn via a DLT source/pipeline
  - Custom DLT source automatically handles REST requests, pagination, data extraction and relational DB storage
  - Predefined endpoints/available datasets include:
    - `get_companies`: scrape followed companies via GraphQL profile components
    - `get_job_urls`: fetch job cards per company
    - `get_descriptions`: fetch job descriptions and details
  - Easily extended to pull additional data 

## Pipeline Flow

The pipeline operates in three main stages:

1. **Pull Followed Companies**  
   It begins by authenticating with LinkedIn (using your session cookies) and fetching the list of companies that the input profile follows.

2. **Fetch Job Listings per Company**  
   For each followed company, the pipeline retrieves all currently posted jobs, collecting job IDs, titles, locations, and other summary information.

3. **(Optional) Fetch Job Descriptions & Insights**  
   If enabled, the pipeline then iterates over the collected job listings to pull detailed job descriptions, requirements, and additional insights for each job.

This modular flow allows you to extract just the company/job listing data, or to enrich it with full job details as needed.


## Set-up
See [docs/setup.md](docs/setup.md)


## Usage

The pipeline pulls in three phases depending on flags:
- **followed companies** → each company's **job listings** → each job's **job details**

Use the built-in entrypoint in `pipeline/voyager_pipeline.py` or call `run_pipeline` directly.

### Quick start (default data)
```bash
python pipeline/voyager_pipeline.py
```

This default run in the `__main__` section fetches followed companies and their job listings. You can modify flags there or call programmatically:

```python
from voyager_pipeline import run_pipeline

run_pipeline(
    db_name="linkedin.duckdb",
    one_at_a_time=False,
    get_companies=True,      # pull followed companies
    get_job_urls=True,       # pull jobs for those companies
    get_descriptions=False   # optionally pull job description details
)
```

Notes:
- The pipeline writes into DuckDB dataset `linkedin_data` inside `linkedin.duckdb`.
- The code uses a custom paginator to iterate Voyager responses responsibly (`avoid_ban`).
- If you encounter import path issues, ensure your working directory is the repository root and that Python can import the `pipeline` package (e.g., by setting `PYTHONPATH=$PWD`).

## Outputs

- **DuckDB**: `linkedin.duckdb` contains tables like:
  - `linkedin_data.followed_companies`
  - `linkedin_data.jobs_by_company`
  - `linkedin_data.job_description`

- **CSV exports**: Generated by helpers in `pipeline/analytics/saved_queries.py` (e.g., `generate_job_urls(..., as_csv=True)`) and saved to the current working directory.


## Analytics & Post-processing

Helpers in `pipeline/analytics/saved_queries.py` allow quick filtering and exports from DuckDB, for example:
- **`get_software_jobs(db_path)`**: Software roles
- **`get_jobs_filtered(db_path, filter_str)`**: Custom DuckDB SQL filter
- **`generate_job_urls(db_path, filter_str, as_csv=True, create_table=False)`**: Build LinkedIn job URLs and optionally export CSV or create a helper table

Additional utilities:
- **`gql_utils.py`**: Build LinkedIn Voyager GraphQL URLs and extract nested payload fragments


## Ethics & Legal

Scraping LinkedIn may violate their Terms of Service and/or applicable laws. Use this project for personal research and educational purposes only. Respect robots.txt, rate limits, and user privacy. You are solely responsible for compliance.

## Troubleshooting
- **Auth issues**: Log out/in on the browser, clear “remembered devices”, and re-run if you hit repeated challenges.
- **Rate limiting**: Increase delays, reduce batch sizes, or limit scope.
- **Import errors**: Run from the repo root and set `PYTHONPATH=$PWD`. If needed, adjust imports to your environment.
- **Redis unavailable**: Cookie caching is optional; the client falls back gracefully.
