## LinkedIn Scraper

A practical LinkedIn data scraper and pipeline based on dlt (data load tool), DuckDB, and lightweight request clients. Created by reverse-engineering LinkedIn's 'hidden' Voyager API endpoints (GraphQL/REST) with cookie-based auth. It focuses on extracting jobs posted for followed companies.

## Key Dependencies
- DLT
- DuckDB
- Streamlit (optional)

### Basic capabilities
  - Easy extraction of data from LinkedIn via a DLT source/pipeline
  - Custom DLT source automatically handles REST requests, pagination, data extraction and relational DB storage
  - Predefined endpoints/available datasets include:
    - `followed_companies`: scrape followed companies via GraphQL profile components
    - `jobs_by_company`: fetch job cards per company
    - `job_description`: fetch job descriptions and details
    - `company_details`: fetch detailed company information including industry, size, headquarters, description, and other company metadata
  - Easily extended to pull additional data

## Pipeline Flow


The pipeline operates in three main stages:

1. **Pull Followed Companies**
   It begins by authenticating with LinkedIn (using your session cookies) and fetching the list of companies that the input profile follows.

2. **Fetch Company Details**
    For each company, the pipeline gathers key information from the profile page. this data includes (but is not limited to) industry, size, headquarters, locations, description, similar companies, affilated companies, etc.

2. **Fetch Job Listings per Company**
   For each followed company, the pipeline retrieves all currently posted jobs, collecting job IDs, titles, locations, and other summary information.

3. **(Optional) Fetch Job Descriptions & Insights**
   If enabled, the pipeline then iterates over the collected job listings to pull detailed job descriptions, requirements, and additional insights for each job.

This modular flow allows you to extract just the company/job listing data, or to enrich it with full job details as needed.


## Set-up
See [docs/setup.md](docs/setup.md)


## Usage

The pipeline pulls data in phases based on the `resources_requested` parameter:
- **followed companies** → each company's **job listings** → each job's **job details**
- **company details** → detailed company information (can be run independently)

Use the built-in entrypoint in `pipeline/voyager_pipeline.py` or call `run_pipeline` directly.

### Quick start (default data)
```bash
python pipeline/voyager_pipeline.py
```

This default run in the `__main__` section fetches followed companies and their job listings. You can modify flags there or call programmatically:

```python
from voyager_pipeline import run_pipeline

run_pipeline(
    db_name="linkedin.duckdb",
    resources_requested=["followed_companies", "jobs_by_company"],  # specify which endpoints to run
    inspect_response=False,  # set to True for debugging API responses
    resource_data={}  # optional: provide existing data to avoid re-fetching
)
```

Notes:
- The pipeline writes into DuckDB dataset `linkedin_data` inside `linkedin.duckdb`.
- The code uses a custom paginator to iterate Voyager responses responsibly (`avoid_ban`).
- Authentication requires `LINKEDIN_USERNAME` and `LINKEDIN_PASSWORD` environment variables.
- If you encounter import path issues, ensure your working directory is the repository root and that Python can import the `pipeline` package (e.g., by setting `PYTHONPATH=$PWD`).

## Outputs

- **DuckDB**: `linkedin.duckdb` contains tables like:
  - `linkedin_data.followed_companies`
  - `linkedin_data.jobs_by_company`
  - `linkedin_data.job_description`
  - `linkedin_data.company_details`

- **CSV exports**: Generated by helpers in `pipeline/analytics/saved_queries.py` (e.g., `generate_job_urls(..., as_csv=True)`) and saved to the current working directory.


## Analytics & Post-processing

Helpers in `pipeline/analytics/saved_queries.py` allow quick filtering and exports from DuckDB, for example:
- **`get_software_jobs(db_path)`**: Software roles
- **`get_data_jobs(db_path)`**: Data engineering/science roles
- **`get_rs_jobs(db_path)`**: Remote sensing/geospatial roles
- **`get_jobs_filtered(db_path, filter_str)`**: Custom DuckDB SQL filter
- **`generate_job_urls(jobs_df, file_name, as_csv=True)`**: Build LinkedIn job URLs and optionally export CSV

Additional utilities:
- **`gql_utils.py`**: Build LinkedIn Voyager GraphQL URLs and extract nested payload fragments


## Ethics & Legal

Scraping LinkedIn may violate their Terms of Service and/or applicable laws. Use this project for personal research and educational purposes only. Respect robots.txt, rate limits, and user privacy. You are solely responsible for compliance.

## Troubleshooting
- **Auth issues**: Ensure `LINKEDIN_USERNAME` and `LINKEDIN_PASSWORD` environment variables are set correctly. Log out/in on the browser, clear "remembered devices", and re-run if you hit repeated challenges.
- **Rate limiting**: Increase delays, reduce batch sizes, or limit scope.
- **Import errors**: Run from the repo root and set `PYTHONPATH=$PWD`. If needed, adjust imports to your environment.
- **Redis unavailable**: Cookie caching is optional; the client falls back gracefully.
