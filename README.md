## LinkedIn Scraper

A practical LinkedIn data scraper and pipeline based on dlt (data load tool), DuckDB, and lightweight request clients. Created by reverse-engineering LinkedIn's 'hidden' Voyager API endpoints (GraphQL/REST) with cookie-based auth. It focuses on extracting jobs posted for followed companies.

## Key Dependencies
- DLT
- DuckDB
- Streamlit (optional)

### Basic capabilities
  * Easy extraction of data from LinkedIn via a DLT source/pipeline
    * Handles authentication, building REST requests, pagination of responses, data   extraction and relational DB storage
  * DuckDB destination w/ Marimo UI for exploration of extracted data
  * Predefined endpoints/available datasets include:
    * `followed_companies`: scrape followed companies via GraphQL profile components
    * `jobs_by_company`: fetch job cards per company
    * `job_description`: fetch job descriptions and details
    * `company_details`: fetch detailed company information including industry, size, headquarters, description, and other company metadata
  * Easily extended to pull additional data (See [configuration](docs/pipeline/configuration.md))

## Resources
The pipeline currently offers four main endpoints:

1. **Followed Companies**
   It begins by authenticating with LinkedIn (using your session cookies) and fetching the list of companies that the input profile follows.

2. **Company Details**
    For each company, the pipeline gathers key information from the profile page. this data includes (but is not limited to) industry, size, headquarters, locations, description, similar companies, affilated companies, etc.

2. **Job Listings (by Company)**
   For each followed company, the pipeline retrieves all currently posted jobs, collecting job IDs, titles, locations, and other summary information.

3. **Job Descriptions (& Meta Data)**
   If enabled, the pipeline then iterates over the collected job listings to pull detailed job descriptions, requirements, and additional insights for each job.

This modular flow allows you to extract just the company/job listing data, or to enrich it with full job details as needed.


## Set-up

See [docs/setup.md](docs/pipeline/setup.md)


## Usage

The pipeline pulls data in phases based on the `resources_requested` parameter. Most data is optional to pull, but there are a few 'source' endpoints that will be used to inform what data is pulled for the other endpoints.

### Populate Source Data
* **followed companies**
  * pulled first as we need company ids to request company details or get company job listings

### Choose Your Resources
- **company details** → detailed company information
- **job listngs** → minimal job data - company, job description, job location, job id
- **job description** → a wide variety of job metadata -


Use the built-in entrypoint in `pipeline/voyager_pipeline.py` or call `run_pipeline` directly.

### Quick start (default data)
```bash
python pipeline/voyager_pipeline.py
```

This default run in the `__main__` section fetches followed companies and their job listings. You can modify flags there or call programmatically:

```python
from voyager_pipeline import run_pipeline

run_pipeline(
    db_name="linkedin.duckdb",
    resources_requested=["followed_companies",
                           "jobs_by_company",
                           "job_descriptions"],  # specify which endpoints to run
    inspect_response=False,  # set to True for debugging API responses
    resource_data={}  # optional: provide existing data to avoid re-fetching
)
```

Notes:
- The pipeline writes into DuckDB dataset `linkedin_data` inside `linkedin.duckdb`.
- The code uses a custom paginator to iterate Voyager responses responsibly (`avoid_ban`).
- Authentication requires `LINKEDIN_USERNAME` and `LINKEDIN_PASSWORD` environment variables.

## Outputs

- **DuckDB**: `linkedin.duckdb` contains tables like:
  - `linkedin_data.followed_companies`
  - `linkedin_data.jobs_by_company`
  - `linkedin_data.job_description`
  - `linkedin_data.company_details`

- **CSV exports**: Generated by helpers in `pipeline/analytics/saved_queries.py` (e.g., `generate_job_urls(..., as_csv=True)`) and saved to the current working directory.


## Analytics & Post-processing

Helpers in `pipeline/analytics/saved_queries.py` allow quick filtering and exports from DuckDB, for example:
- **`get_software_jobs(db_path)`**: Software roles
- **`get_data_jobs(db_path)`**: Data engineering/science roles
- **`get_rs_jobs(db_path)`**: Remote sensing/geospatial roles
- **`get_jobs_filtered(db_path, filter_str)`**: Custom DuckDB SQL filter
- **`generate_job_urls(jobs_df, file_name, as_csv=True)`**: Build LinkedIn job URLs and optionally export CSV

Additional utilities:
- **`gql_utils.py`**: Build LinkedIn Voyager GraphQL URLs and extract nested payload fragments


## Ethics & Legal

Scraping LinkedIn may violate their Terms of Service and/or applicable laws. Use this project for personal research and educational purposes only. Respect robots.txt, rate limits, and user privacy. You are solely responsible for compliance.

## Troubleshooting
- **Auth issues**: Ensure `LINKEDIN_USERNAME` and `LINKEDIN_PASSWORD` environment variables are set correctly. Log out/in on the browser, clear "remembered devices", and re-run if you hit repeated challenges.
- **Rate limiting**: Increase delays, reduce batch sizes, or limit scope.
- **Import errors**: Run from the repo root and set `PYTHONPATH=$PWD`. If needed, adjust imports to your environment.
- **Redis unavailable**: Cookie caching is optional; the client falls back gracefully.
